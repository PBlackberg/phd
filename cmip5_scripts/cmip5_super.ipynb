{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d833bde3-3857-4ab7-99d4-c3f7bb088981",
   "metadata": {},
   "source": [
    "## CMIP5 - precipitation extremes and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8f1b0-9582-48e1-b8c9-b33fdd57a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import skimage.measure as skm\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58d019-50dc-4b9d-af9d-b72429714d34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab17e2-da72-41ad-9824-77737ea17a6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### regridder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5992f-c90b-4b06-afd2-d5a495c08220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = intake.cat.nci['esgf'].cmip5.search(\n",
    "                                        model_id = 'FGOALS-g2', \n",
    "                                        experiment = 'historical',\n",
    "                                        time_frequency = 'day', \n",
    "                                        realm = 'atmos', \n",
    "                                        ensemble = 'r1i1p1', \n",
    "                                        variable= 'pr').to_dataset_dict()\n",
    "\n",
    "ds_regrid = ds_dict[list(ds_dict.keys())[-1]].sel(time='1970-01-01', lon=slice(0,360),lat=slice(-30,30))\n",
    "\n",
    "\n",
    "def regridder(ds_orig):\n",
    "    ds_out = xr.Dataset(\n",
    "        {\n",
    "            \"lat\": ([\"lat\"], ds_regrid.lat.data),\n",
    "            \"lon\": ([\"lon\"], ds_regrid.lon.data),\n",
    "        }\n",
    "        )\n",
    "    regrid = xe.Regridder(ds_orig, ds_out, 'bilinear', periodic=True)\n",
    "    \n",
    "    return regrid(ds_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb0602-1334-4310-8220-5f44bee99f42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### pr_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67282da7-5852-41f8-9023-f48ca22e2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_examples(var3d):\n",
    "    # snapshot of daily scene\n",
    "    pr_day = var3d.isel(time=0)\n",
    "    \n",
    "    # time mean of precipitation\n",
    "    pr_tMean= var3d.mean(dim='time', keep_attrs=True)\n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_pr_example_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)    \n",
    "\n",
    "        xr.Dataset({'pr_day': pr_day, 'pr_tMean': pr_tMean}).to_netcdf(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5e460-a54e-4e38-83fd-cb4b6829f51a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### rx1day, rx5day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789c13e-c678-4571-8942-23398cc46c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rxday(var3d):\n",
    "    rx1day = var3d.resample(time='Y').max(dim='time')\n",
    "    \n",
    "    precip5day = var3d.resample(time='5D').mean(dim='time')\n",
    "    rx5day = precip5day.resample(time='Y').max(dim='time')\n",
    "    \n",
    "    # .mean(dim=('time'),keep_attrs=True)\n",
    "    # .mean(dim=('lat','lon'),keep_attrs=True)\n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_pr_rxday_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "        xr.Dataset({'rx1day': rx1day, 'rx5day': rx5day}).to_netcdf(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cab238-faea-4c3d-820a-753fc24dab0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### extreme percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cb608-12f6-4802-a2fd-56e26c12f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_percentiles(var3d):\n",
    "\n",
    "    pr_97 = var3d.quantile(0.97,dim=('lat','lon'),keep_attrs=True)\n",
    "    pr_97 = pr_97.drop('quantile',dim=None)\n",
    "\n",
    "\n",
    "    pr_99 = var3d.quantile(0.99,dim=('lat','lon'),keep_attrs=True)\n",
    "    pr_99 = pr_99.drop('quantile',dim=None)\n",
    "\n",
    "\n",
    "    pr_999 = var3d.quantile(0.999,dim=('lat','lon'),keep_attrs=True)\n",
    "    pr_999 = pr_999.drop('quantile',dim=None)\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_pr_extreme_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "        xr.Dataset({'pr_97': pr_97, 'pr_99': pr_99, 'pr_999': pr_999}).to_netcdf(path) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695c18b-7a3f-4e16-bec2-9a5f73ad36f7",
   "metadata": {},
   "source": [
    "### convective object properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895ac57-92a1-48ee-a0d8-e8906800bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects that touch across lon=0, lon=360 boundary are the same object, array(lat, lon)\n",
    "def connect_boundary(array):\n",
    "    s = np.shape(array)\n",
    "    for row in np.arange(0,s[0]):\n",
    "        if array[row,0]>0 and array[row,-1]>0:\n",
    "            array[array==array[row,0]] = min(array[row,0],array[row,-1])\n",
    "            array[array==array[row,-1]] = min(array[row,0],array[row,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50bcc1-77fa-4e5f-9510-f8d36db59bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_props(var3d, aream, latm, lonm, lat, lon): #(lon in 0-360)\n",
    "    conv_threshold = var3d.quantile(0.97,dim=('lat','lon')).mean(dim=('time'))\n",
    "    \n",
    "    o_pr, o_area = [], []\n",
    "    for i in range(n_days): #len(precip.time)):\n",
    "        \n",
    "        pr_day = np.expand_dims(var3d.isel(time=i),axis=2)\n",
    "        \n",
    "        L = skm.label(var3d.isel(time=i).where(var3d.isel(time=i)>=conv_threshold,0)>0, background=0,connectivity=2)\n",
    "        connect_boundary(L)\n",
    "        labels = np.unique(L)[1:]\n",
    "    \n",
    "        obj3d = np.stack([(L==label) for label in labels],axis=2)*1\n",
    "                \n",
    "        o_pr = np.append(o_pr, np.sum(obj3d * pr_day * aream, axis=(0,1)) / np.sum(obj3d*aream, axis=(0,1)))\n",
    "        o_area = np.append(o_area, np.sum(obj3d * aream, axis=(0,1)))    \n",
    "                                 \n",
    "            \n",
    "    o_pr = xr.DataArray(o_pr, attrs=dict(description=\"area weighted mean pr in object\", units=\"mm/day\"))\n",
    "    o_area = xr.DataArray(o_area, attrs=dict(description=\"area of object\", units=\"km$^2$\"))\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_pr_objects_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    \n",
    "        xr.Dataset({'o_pr': o_pr, 'o_area': o_area}).to_netcdf(path) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603d54e-e891-4e19-af41-1261e2761ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### aggregation index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb4d5b-7db4-443b-8b51-c86c981b8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great circle distance (Haversine formula) (lon in 0-360)\n",
    "def hav_dist(lat1, lon1, lat2, lon2, R):\n",
    "\n",
    "    # radius of earth in km\n",
    "    #R = 6373.0\n",
    "\n",
    "    lat1 = np.deg2rad(lat1)                       \n",
    "    lon1 = np.deg2rad(lon1-180)     \n",
    "    lat2 = np.deg2rad(lat2)                       \n",
    "    lon2 = np.deg2rad(lon2-180)\n",
    "\n",
    "    # Haversine formula\n",
    "    h = np.sin((lat2 - lat1)/2)**2 + np.cos(lat1)*np.cos(lat2) * np.sin((lon2 - lon1)/2)**2\n",
    "\n",
    "    # distance from Haversine function:\n",
    "    # h = sin(theta/2)^2\n",
    "    # central angle, theta:\n",
    "    # theta = (great circle distance) / radius \n",
    "    # d = R * sin^-1(sqrt(h))*2 \n",
    "\n",
    "    return 2 * R * np.arcsin(np.sqrt(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38a903-6e4e-4b55-83d1-6c3c0861a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rome(array, labels, aream, latm, lonm, lat, lon, R):\n",
    "    sL = np.shape(array)\n",
    "    ROME_allPairs = []\n",
    "        \n",
    "    if len(labels) ==1:\n",
    "        ROME_allPairs = np.sum((array==labels)*1 * aream)\n",
    "\n",
    "    else:\n",
    "        for idx, labeli in enumerate(labels[0:-1]):\n",
    "            \n",
    "            # find coordinates of object i\n",
    "            I, J = zip(*np.argwhere(array==labeli))\n",
    "            I = list(I)\n",
    "            J = list(J)\n",
    "\n",
    "            # area of object i\n",
    "            oi_area = np.sum(np.squeeze(aream)[I,J])\n",
    "\n",
    "            # shortest distance from object i        \n",
    "            # count the number of gridboxes\n",
    "            Ni = len(I)\n",
    "\n",
    "            # replicate each gridbox lon and lat to Ni 2D slices the shape of L\n",
    "            lati3d = np.tile(lat[I],reps =[sL[0], sL[1], 1])\n",
    "            loni3d = np.tile(lon[J],reps =[sL[0], sL[1], 1])\n",
    "\n",
    "            # create corresponding 3D matrix from Ni copies of \n",
    "            # the mesh grid lon, lat, this metrix only needs to \n",
    "            # be recreated when Ni increases from previous loop\n",
    "            if Ni > np.shape(lonm)[2]:\n",
    "                lonm = np.tile(lonm[:,:,0:1],reps =[1, 1, Ni])\n",
    "                latm = np.tile(latm[:,:,0:1],reps =[1, 1, Ni])\n",
    "            # Otherwise you can index the previously created matrix to match lati3d, loni3d\n",
    "\n",
    "            # distance from gridbox to every other point in the domain\n",
    "            p_hav = hav_dist(lati3d,loni3d,latm[:,:,0:Ni],lonm[:,:,0:Ni], R)\n",
    "\n",
    "            # minimum in the third dimension gives shortest distance from \n",
    "            # object i to every other point in the domain\n",
    "            p_dist = np.amin(p_hav, axis=2)\n",
    "\n",
    "            # pick out desired coordinates of p_dist, from the coordinates of the\n",
    "            # unique pair object j\n",
    "            # the minimum of the coordinates in p_dist will be the shortest distance.\n",
    "            for labelj in labels[idx+1:]:\n",
    "\n",
    "                # coordinates of object j\n",
    "                I, J = zip(*np.argwhere(array==labelj))\n",
    "\n",
    "                # area of object j\n",
    "                oj_area = np.sum(np.squeeze(aream)[I,J])\n",
    "\n",
    "                # ROME of unique pair\n",
    "                large_area = np.maximum(oi_area, oj_area)\n",
    "                small_area = np.maximum(oi_area, oj_area)\n",
    "                ROME_pair = large_area + np.minimum(small_area, (small_area/np.amin(p_dist[I,J]))**2)\n",
    "                ROME_allPairs = np.append(ROME_allPairs, ROME_pair)\n",
    "            \n",
    "    return np.mean(ROME_allPairs)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749cd85-e6c0-477f-90f8-b116b819a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_index(var3d, aream, latm, lonm, lat, lon, R, n_days, model):\n",
    "    conv_threshold = var3d.quantile(0.97,dim=('lat','lon')).mean(dim=('time'))\n",
    "    \n",
    "    n = 8\n",
    "    aWeights = np.cos(np.deg2rad(var3d.lat))\n",
    "    aWeights.name = \"weights\"\n",
    "    \n",
    "    scene_oNumber, scene_areaf, ROME, ROME_n = [], [], [], []\n",
    "    for i in range(n_days): #len(precip.time)):\n",
    "        \n",
    "        # if var3d.time[i].dt.strftime('%b') == 'Jan':\n",
    "        #     print(var3d.time[i].dt.strftime('%Y' '/' '%m'))\n",
    "        \n",
    "        \n",
    "        conv_day = (var3d.isel(time=i).where(var3d.isel(time=i)>=conv_threshold,0)>0)*1\n",
    "        \n",
    "        L = skm.label(conv_day, background=0, connectivity=2)\n",
    "        connect_boundary(L)\n",
    "        \n",
    "        scene_areaf.append(conv_day.weighted(aWeights).mean(dim=('lat','lon')))\n",
    "        \n",
    "        labels_all = np.unique(L)[1:]\n",
    "        scene_oNumber.append(len(labels_all))\n",
    "        \n",
    "        ROME.append(calculate_rome(L, labels_all, aream, latm, lonm, lat, lon, R))\n",
    "        \n",
    "        # n largest objects (8)\n",
    "        # index of n largest objects in L        \n",
    "        if len(labels_all) <= n:\n",
    "            labels_n = labels\n",
    "        else:\n",
    "            obj3d = np.stack([(L==label) for label in labels_all],axis=2)*1\n",
    "            o_areaL = np.sum(obj3d * aream, axis=(0,1))\n",
    "            labels_n = labels_all[o_areaL.argsort()[-n:]]\n",
    "                    \n",
    "        ROME_n.append(calculate_rome(L,labels_n, aream, latm, lonm, lat, lon, R))\n",
    "\n",
    "                    \n",
    "    scene_oNumber = xr.DataArray(scene_oNumber)\n",
    "    scene_areaf = xr.DataArray(scene_areaf, attrs=dict(description=\"areafraction of convection from percentile threshold\"))\n",
    "    ROME = xr.DataArray(ROME)\n",
    "    ROME_n = xr.DataArray(ROME_n, attrs=dict(description=\"ROME calculated from n= {} largest objects in scene\".format(n)))\n",
    "    \n",
    "                    \n",
    "    if save:\n",
    "        fileName = model + '_pr_aggScene_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "        xr.Dataset({'scene_oNumber': scene_oNumber, 'ROME': ROME, 'ROME_n': ROME_n, 'scene_areaf': scene_areaf}).to_netcdf(path) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc66f2-cb70-46f9-ba4e-1128f2a3c892",
   "metadata": {},
   "source": [
    "### tas_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1b555-b1a6-4517-aa35-0c57e86f8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tas_examples(var3d):\n",
    "    # snapshot of daily scene\n",
    "    tas_day = var3d.isel(time=0)\n",
    "    \n",
    "    # time mean of precipitation\n",
    "    tas_tMean= var3d.mean(dim='time', keep_attrs=True)\n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_tas_example_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)    \n",
    "\n",
    "        xr.Dataset({'tas_day': tas_day, 'tas_tMean': tas_tMean}).to_netcdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6fd7f-3306-4041-a769-68fdc8953f12",
   "metadata": {},
   "source": [
    "### hus examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c0b80-3dd5-43fd-b0e4-e9ea988e33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hus_examples(var3d, model):\n",
    "\n",
    "    if model == 'IPSL-CM5A-MR':\n",
    "        # snapshot of daily scene\n",
    "        hus_day = var3d.isel(time=0).mean(dim='lev',keep_attrs=True)\n",
    "\n",
    "        # time mean of precipitation\n",
    "        hus_tMean= var3d.mean(dim=('lev','time'), keep_attrs=True)\n",
    "        \n",
    "    else:\n",
    "        # snapshot of daily scene\n",
    "        hus_day = var3d.isel(time=0).mean(dim='plev',keep_attrs=True)\n",
    "\n",
    "        # time mean of precipitation\n",
    "        hus_tMean= var3d.mean(dim=('plev','time'), keep_attrs=True)\n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_hus_example_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)    \n",
    "\n",
    "        xr.Dataset({'hus_day': hus_day, 'hus_tMean': hus_tMean}).to_netcdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67b0c4-f8e1-404d-ab1f-58a5f32d3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'a'\n",
    "if a == 'a':\n",
    "    print('executes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6cdc8f-41bb-48bf-9f80-78d08e01bbab",
   "metadata": {},
   "source": [
    "### hus daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cafa4d-aee3-4945-8989-cba90e56ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hus_daily(var3d, lat):\n",
    "    # spatial mean\n",
    "    aWeights = np.cos(np.deg2rad(lat))\n",
    "    aWeights.name = \"aWeights\"\n",
    "    \n",
    "    if model == 'IPSL-CM5A-MR':\n",
    "        hus_sMean= var3d.weighted(aWeights).mean(dim=('lev','lat','lon'), keep_attrs=True)\n",
    "        \n",
    "    else:\n",
    "        hus_sMean= var3d.weighted(aWeights).mean(dim=('plev','lat','lon'), keep_attrs=True)\n",
    "    \n",
    "    if save:\n",
    "        fileName = model + '_hus_daily_' + experiment + '.nc'\n",
    "        path = folder + '/' + fileName\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)    \n",
    "\n",
    "        xr.Dataset({'hus_sMean': hus_sMean}).to_netcdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa52480-20d3-45db-87e9-5adc4fa60c56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f04261-bd9f-4919-a653-0416917c19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        # 'IPSL-CM5A-MR', # 1 # hus lev instead of plev\n",
    "        # 'GFDL-CM3',     # 2\n",
    "        # 'GISS-E2-H',    # 3\n",
    "        # 'bcc-csm1-1',   # 4\n",
    "        # 'CNRM-CM5',     # 5\n",
    "        # 'CCSM4',        # 6 # hus possibly wrong ensemble\n",
    "        # 'HadGEM2-AO',   # 7\n",
    "        # 'BNU-ESM',      # 8 # tas did not work\n",
    "        'EC-EARTH',     # 9 # tas did not work\n",
    "        'FGOALS-g2',    # 10 # tas slicing\n",
    "        'MPI-ESM-MR',   # 11\n",
    "        'CMCC-CM',      # 12\n",
    "        'inmcm4',       # 13\n",
    "        'NorESM1-M',    # 14\n",
    "        'CanESM2',      # 15 # pr indexing time period\n",
    "        'MIROC5',       # 16\n",
    "        'HadGEM2-CC',   # 17\n",
    "        'MRI-CGCM3',    # 18\n",
    "        'CESM1-BGC'     # 19\n",
    "        ]\n",
    "\n",
    "historical = True\n",
    "rcp85 = False\n",
    "\n",
    "pr_examples = False\n",
    "rxday = False\n",
    "high_percentiles = False\n",
    "object_props = False\n",
    "aggregation_index = False\n",
    "\n",
    "tas_examples = False\n",
    "\n",
    "hus_examples = True\n",
    "hus_daily = True\n",
    "\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23862c9b-9ab9-4dd2-92af-0fc7a654b39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # precipitation extremes and aggregation\n",
    "# for model in models:\n",
    "#     start = timeit.default_timer()\n",
    "\n",
    "#     ensemble = 'r1i1p1'\n",
    "    \n",
    "#     if historical:\n",
    "#         experiment = 'historical'\n",
    "#         period=slice('1970-01','1999-12') #'1970-01-01','1999-12-31'\n",
    "\n",
    "\n",
    "#         if model == 'GISS-E2-H':\n",
    "#             ensemble = 'r6i1p1'\n",
    "\n",
    "\n",
    "#     if rcp85:\n",
    "#         experiment = 'rcp85'\n",
    "#         period=slice('2070-01','2099-12')\n",
    "\n",
    "#         if model == 'GISS-E2-H':\n",
    "#             ensemble = 'r2i1p1'\n",
    "\n",
    "\n",
    "\n",
    "#     ds_dict = intake.cat.nci['esgf'].cmip5.search(\n",
    "#                                             model_id = model, \n",
    "#                                             experiment = experiment,\n",
    "#                                             time_frequency = 'day', \n",
    "#                                             realm = 'atmos', \n",
    "#                                             ensemble = ensemble, \n",
    "#                                             variable= 'pr').to_dataset_dict()\n",
    "\n",
    "#     ds_pr = ds_dict[list(ds_dict.keys())[-1]].sel(time=period, lon=slice(0,360),lat=slice(-30,30))\n",
    "#     precip = regridder(ds_pr).pr*60*60*24\n",
    "#     precip.attrs['units']= 'mm/day'\n",
    "#     n_days = 2 #len(precip.time)\n",
    "\n",
    "#     R = 6371.0 #km\n",
    "#     lat = precip.lat\n",
    "#     lon = precip.lon\n",
    "#     lonm, latm = np.meshgrid(lon, lat)\n",
    "#     dlon = lon[1]-lon[0]\n",
    "#     dlat = lat[1]-lat[0]\n",
    "#     aream = np.cos(np.deg2rad(latm))*np.float64(dlon*dlat*R**2*(np.pi/180)**2)\n",
    "    \n",
    "#     aream = np.expand_dims(aream,axis=2)\n",
    "#     latm = np.expand_dims(latm,axis=2)\n",
    "#     lonm = np.expand_dims(lonm,axis=2)\n",
    "\n",
    "\n",
    "#     folder = '/g/data/k10/cb4968/cmip5/' + model\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\n",
    "#     if pr_examples:\n",
    "#         get_pr_examples(precip)\n",
    "\n",
    "#     if rxday:\n",
    "#         get_rxday(precip)\n",
    "\n",
    "#     if high_percentiles:\n",
    "#         get_high_percentiles(precip)\n",
    "\n",
    "#     if object_props:\n",
    "#         get_object_props(precip, aream, latm, lonm, lat, lon)\n",
    "\n",
    "#     if aggregation_index:    \n",
    "#         get_aggregation_index(precip, aream, latm, lonm, lat, lon, R, n_days, model)\n",
    "    \n",
    "\n",
    "\n",
    "#     stop = timeit.default_timer()\n",
    "#     print('model: {} took {} minutes to finsih'.format(model, (stop-start)/60))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630b5d6-cbf7-44f5-94c8-60babbec7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tas\n",
    "# for model in models:\n",
    "#     start = timeit.default_timer()\n",
    "\n",
    "#     if historical:\n",
    "#         experiment = 'historical'\n",
    "#         period=slice('1970-01','1999-12') #'1970-01-01','1999-12-31'\n",
    "#         ensemble = 'r1i1p1'\n",
    "\n",
    "#         if model == 'GISS-E2-H':\n",
    "#             ensemble = 'r6i1p1'\n",
    "\n",
    "\n",
    "#     if rcp85:\n",
    "#         experiment = 'rcp85'\n",
    "#         period=slice('2070-01','2099-12')\n",
    "\n",
    "#         if model == 'GISS-E2-H':\n",
    "#             ensemble = 'r2i1p1'\n",
    "\n",
    "\n",
    "\n",
    "#     ds_dict = intake.cat.nci['esgf'].cmip5.search(\n",
    "#                                             model_id = model, \n",
    "#                                             experiment = experiment,\n",
    "#                                             time_frequency = 'mon', \n",
    "#                                             realm = 'atmos', \n",
    "#                                             ensemble = ensemble, \n",
    "#                                             variable= 'tas').to_dataset_dict()\n",
    "\n",
    "#     ds_tas = ds_dict[list(ds_dict.keys())[-1]].sel(time=period, lon=slice(0,360),lat=slice(-30,30))\n",
    "#     tas = regridder(ds_tas).tas - 273.15\n",
    "#     tas.attrs['units']= 'deg(C)'\n",
    "#     n_days = 2 #len(precip.time)\n",
    "\n",
    "#     R = 6371.0 #km\n",
    "#     lat = tas.lat\n",
    "#     lon = tas.lon\n",
    "#     lonm, latm = np.meshgrid(lon, lat)\n",
    "#     dlon = lon[1]-lon[0]\n",
    "#     dlat = lat[1]-lat[0]\n",
    "#     aream = np.cos(np.deg2rad(latm))*np.float64(dlon*dlat*R**2*(np.pi/180)**2)\n",
    "    \n",
    "#     aream = np.expand_dims(aream,axis=2)\n",
    "#     latm = np.expand_dims(latm,axis=2)\n",
    "#     lonm = np.expand_dims(lonm,axis=2)\n",
    "\n",
    "\n",
    "#     folder = '/g/data/k10/cb4968/cmip5/' + model\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\n",
    "#     if tas_examples:\n",
    "#         get_tas_examples(tas)\n",
    "    \n",
    "\n",
    "\n",
    "#     stop = timeit.default_timer()\n",
    "#     print('model: {} took {} minutes to finsih'.format(model, (stop-start)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b8961-d65a-4746-b145-eb53dfa9662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hus\n",
    "for model in models:\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    if historical:\n",
    "        experiment = 'historical'\n",
    "        period=slice('1970-01','1999-12') #'1970-01-01','1999-12-31'\n",
    "        ensemble = 'r1i1p1'\n",
    "\n",
    "        if model == 'GISS-E2-H':\n",
    "            ensemble = 'r6i1p1'\n",
    "\n",
    "\n",
    "    if rcp85:\n",
    "        experiment = 'rcp85'\n",
    "        period=slice('2070-01','2099-12')\n",
    "\n",
    "        if model == 'GISS-E2-H':\n",
    "            ensemble = 'r2i1p1'\n",
    "\n",
    "\n",
    "\n",
    "    ds_dict = intake.cat.nci['esgf'].cmip5.search(\n",
    "                                            model_id = model, \n",
    "                                            experiment = experiment,\n",
    "                                            time_frequency = 'day', \n",
    "                                            realm = 'atmos', \n",
    "                                            ensemble = ensemble, \n",
    "                                            variable= 'hus').to_dataset_dict()\n",
    "\n",
    "    ds_hus = ds_dict[list(ds_dict.keys())[-1]].sel(time=period, lon=slice(0,360),lat=slice(-30,30))\n",
    "    ds_hus = ds_hus.where(ds_hus.hus < 1e+20) \n",
    "    hus = regridder(ds_hus).hus*1000\n",
    "    hus.attrs['units']= 'g/kg'\n",
    "    n_days = 2 #len(precip.time)\n",
    "\n",
    "    R = 6371.0 #km\n",
    "    lat = hus.lat\n",
    "    lon = hus.lon\n",
    "    lonm, latm = np.meshgrid(lon, lat)\n",
    "    dlon = lon[1]-lon[0]\n",
    "    dlat = lat[1]-lat[0]\n",
    "    aream = np.cos(np.deg2rad(latm))*np.float64(dlon*dlat*R**2*(np.pi/180)**2)\n",
    "    \n",
    "    aream = np.expand_dims(aream,axis=2)\n",
    "    latm = np.expand_dims(latm,axis=2)\n",
    "    lonm = np.expand_dims(lonm,axis=2)\n",
    "\n",
    "\n",
    "    folder = '/g/data/k10/cb4968/cmip5/' + model\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    if hus_examples:\n",
    "        get_hus_examples(hus, model)\n",
    "    \n",
    "    \n",
    "    if hus_daily:\n",
    "        get_hus_daily(hus, lat)\n",
    "\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('model: {} took {} minutes to finsih'.format(model, (stop-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5b6a4",
   "metadata": {},
   "source": [
    "# saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5692330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for saving\n",
    "numberIndex = xr.DataArray(\n",
    "    data = numberIndex, \n",
    "    attrs=dict(description=\"Number of objects in scene\", units=\"Nb\"))\n",
    "\n",
    "areaf = xr.DataArray(\n",
    "    data = areaf, \n",
    "    attrs=dict(description=\"areafraction covered by convection in scene\", units=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for saving\n",
    "o_pr = xr.DataArray(\n",
    "    data = o_pr, \n",
    "    attrs=dict(description=\"area weighted mean pr in object\", units=\"mm/day\"))\n",
    "\n",
    "o_area = xr.DataArray(\n",
    "    data = o_pr, \n",
    "    attrs=dict(description=\"area of object\", units=\"km$^2$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4db3e6-75d8-44a1-a87e-c24eb50fa1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object properties\n",
    "save = False\n",
    "if save:\n",
    "    fileName = model + '_pr_objects_' + experiment + '.nc'\n",
    "    path = folder + '/' + fileName\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "    xr.Dataset({'o_lat': o_lat, 'o_lon': o_lon, 'o_pr': o_pr, 'o_area': o_area}).to_netcdf(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4596c-1908-46c3-8bcf-5ce9a5089e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation index\n",
    "save = False\n",
    "if save:\n",
    "    fileName = model + '_pr_aggScene_' + experiment + '.nc'\n",
    "    path = folder + '/' + fileName\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "    xr.Dataset({'scene_oNumber': scene_oNumber, 'ROME': ROME, 'ROME_n': ROME_n, 'scene_areaf': scene_areaf}).to_netcdf(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069356e-f92f-4a3e-9794-97abd308d079",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efe6dc-3d30-4e94-8fb2-c65c79a5ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = 'GFDL-CM3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234ca17-39a8-4c26-af20-fffeeb65df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_local = xr.open_dataset('/g/data/k10/cb4968/cmip5/' + model + '/' + model + '_pr_example_' + experiment + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f750abe-e01c-4dda-a085-5af72db98ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_local = xr.open_dataset('/g/data/k10/cb4968/cmip5/' + model + '/' + model + '_pr_rxday_' + experiment + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b7d69-f8e3-42b7-b0d8-1a84d7a2b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_local = xr.open_dataset('/g/data/k10/cb4968/cmip5/' + model + '/' + model + '_pr_extreme_' + experiment + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08381898-f571-44ca-91de-3b26be9a2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_local = xr.open_dataset('/g/data/k10/cb4968/cmip5/' + model + '/' + model + '_object_props_' + experiment + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926e6d7-346c-465a-a443-37ff33418222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_local = xr.open_dataset('/g/data/k10/cb4968/cmip5/' + model + '/' + model + '_aggregation_index_' + experiment + '.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
